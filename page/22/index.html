<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>tonglin0325的个人主页</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="tonglin0325的个人主页">
<meta property="og:url" content="http://tonglin0325.github.io/page/22/index.html">
<meta property="og:site_name" content="tonglin0325的个人主页">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="tonglin0325">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="tonglin0325的个人主页" type="application/atom+xml">
  
  
    <link rel="icon" href="https://tonglin0325.github.io/images/favicon.ico">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
<link rel="stylesheet" href="/css/styles.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-166238833-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">主页</a></li>
        
          <li><a class=""
                 href="/archives/">所有文章</a></li>
        
          <li><a class=""
                 href="http://www.cnblogs.com/tonglin0325/">博客园</a></li>
        
          <li><a class=""
                 href="https://github.com/tonglin0325">Github</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">tonglin0325的个人主页</h1>
  
</div>

    <div class="row">
        <div class="col-sm-9 blog-main">
          
  
    <article id="post-使用Python解析JSON数据" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/24/%E4%BD%BF%E7%94%A8Python%E8%A7%A3%E6%9E%90JSON%E6%95%B0%E6%8D%AE/">使用Python解析JSON数据</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/24/%E4%BD%BF%E7%94%A8Python%E8%A7%A3%E6%9E%90JSON%E6%95%B0%E6%8D%AE/" class="article-date"><time datetime="2016-12-23T16:00:00.000Z" itemprop="datePublished">2016-12-24</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>使用Python解析百度API返回的JSON格式的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"># !&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from numpy import *</span><br><span class="line">import sys, urllib, urllib2, json</span><br><span class="line">import fun</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">	dataMat,labelMat &#x3D; fun.loadDataSet(&quot;code.txt&quot;)</span><br><span class="line">	print dataMat[0]</span><br><span class="line">	url &#x3D; &#39;http:&#x2F;&#x2F;apis.baidu.com&#x2F;apistore&#x2F;stockservice&#x2F;usastock?stockid&#x3D;CFC-B&amp;list&#x3D;1&#39;</span><br><span class="line">	req &#x3D; urllib2.Request(url)</span><br><span class="line">	</span><br><span class="line">	req.add_header(&quot;apikey&quot;, &quot;你自己的apikey&quot;)</span><br><span class="line">	resp &#x3D; urllib2.urlopen(req)</span><br><span class="line">	content &#x3D; resp.read()</span><br><span class="line">	s &#x3D; json.loads(content)</span><br><span class="line"></span><br><span class="line">	for i in s.keys():</span><br><span class="line">		if i &#x3D;&#x3D; &#39;errNum&#39;:</span><br><span class="line">			print &#39;错误码：&#39;,s[i]</span><br><span class="line">		elif i &#x3D;&#x3D; &#39;errMsg&#39;:</span><br><span class="line">			print &#39;错误信息：&#39;,s[i]</span><br><span class="line">		else:</span><br><span class="line">			for j in s[i].keys():</span><br><span class="line">				if j &#x3D;&#x3D; &#39;stockinfo&#39;:</span><br><span class="line">					print &quot;返回数据:&quot;,j,&quot;:&quot;,str(s[i][j]).replace(&#39;u\&#39;&#39;,&#39;\&#39;&#39;).decode(&quot;unicode-escape&quot;)+&#39;\n&#39;</span><br><span class="line">				elif j &#x3D;&#x3D; &#39;market&#39;:</span><br><span class="line">					for k in s[i][j].keys():</span><br><span class="line">						print k,&quot;:&quot;,str(s[i][j][k]).replace(&#39;u\&#39;&#39;,&#39;\&#39;&#39;).decode(&quot;unicode-escape&quot;)+&#39;\n&#39;</span><br><span class="line">				#print &quot;返回数据:&quot;,j,&quot;:&quot;,str(s[i][j]).replace(&#39;u\&#39;&#39;,&#39;\&#39;&#39;).decode(&quot;unicode-escape&quot;)</span><br></pre></td></tr></table></figure>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/24/%E4%BD%BF%E7%94%A8Python%E8%A7%A3%E6%9E%90JSON%E6%95%B0%E6%8D%AE/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/24/%E4%BD%BF%E7%94%A8Python%E8%A7%A3%E6%9E%90JSON%E6%95%B0%E6%8D%AE/" data-id="ckca3uy2300cew20l3zal5y9m" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/JSON/" rel="tag">JSON</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习——回归树" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92%E6%A0%91/">机器学习——回归树</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92%E6%A0%91/" class="article-date"><time datetime="2016-12-23T16:00:00.000Z" itemprop="datePublished">2016-12-24</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>**　　线性回归<strong>创建模型需要</strong>拟合所有的样本点<strong>（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂的时候，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是</strong>非线性**的，不可能使用全局限性模型来拟合任何数据。</p>
<p>　　一种可行的方法是将数据集切分成很多份易建模的数据，然后再利用线性回归技术来建模。如果首次切分之后仍然难以拟合线性模型就继续切分。</p>
<p>　　决策树是一种<strong>贪心算法</strong>，它要在给定时间内做出最佳选择，但是<strong>并不关心能否达到全局最优</strong>。</p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92%E6%A0%91/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92%E6%A0%91/" data-id="ckca3uy2g00dcw20l90aoe6ki" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-数据标准化（转）" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/23/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E8%BD%AC%EF%BC%89/">数据标准化（转）</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/23/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E8%BD%AC%EF%BC%89/" class="article-date"><time datetime="2016-12-22T16:00:00.000Z" itemprop="datePublished">2016-12-23</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>数据的<strong>标准化（normalization）</strong>是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p>
<p>　　其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上，常见的数据归一化的方法有：</p>
<h3 id="min-max标准化-Min-max-normalization"><a href="#min-max标准化-Min-max-normalization" class="headerlink" title="min-max标准化(Min-max normalization)"></a><a name="t0"></a>min-max标准化(Min-max normalization)</h3><p>　　也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：</p>
<img src="/images/1350642893_1562.png" alt="" />

<p>　　其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p>
<h3 id="log函数转换"><a href="#log函数转换" class="headerlink" title="log函数转换"></a><a name="t1"></a>log函数转换</h3><p>　　通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：</p>
<img src="/images/1350642937_4388.png" alt="" />

<p>　　看了下网上很多介绍都是x<sup>*</sup>=log<sub>10</sub>(x)，其实是有问题的，这个结果并非一定落到[0,1]区间上，应该还要除以log<sub>10</sub>(max)，max为样本数据最大值，并且所有的数据都要大于等于1。</p>
<h3 id="atan函数转换"><a href="#atan函数转换" class="headerlink" title="atan函数转换"></a><a name="t2"></a>atan函数转换</h3><p>　　用反正切函数也可以实现数据的归一化：</p>
<img src="/images/1350642965_3637.png" alt="" />

<p>　　使用这个方法需要注意的是如果想映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[-1,0]区间上。</p>
<p>　　而并非所有数据标准化的结果都映射到[0,1]区间上，其中最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法：</p>
<h3 id="z-score-标准化-zero-mean-normalization"><a href="#z-score-标准化-zero-mean-normalization" class="headerlink" title="z-score 标准化(zero-mean normalization)"></a><a name="t3"></a>z-score 标准化(zero-mean normalization)</h3><p>　　也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：</p>
<img src="/images/1350642981_3490.png" alt="" />

<p>　　其中&mu;为所有样本数据的均值，&sigma;为所有样本数据的标准差。<br /><br><br /><br><br /></p>
<blockquote>
</blockquote>
<p>转载请注明来源：<a href="http://webdataanalysis.net/" target="_blank" rel="noopener">网站数据分析</a> &raquo;<a href="http://webdataanalysis.net/data-analysis-method/data-normalization/" target="_blank" rel="noopener">《数据的标准化》</a></p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/23/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E8%BD%AC%EF%BC%89/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/23/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E8%BD%AC%EF%BC%89/" data-id="ckca3uy2900d0w20l7n20buzp" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-特征选择和特征理解 （转）" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/23/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E7%89%B9%E5%BE%81%E7%90%86%E8%A7%A3%20%EF%BC%88%E8%BD%AC%EF%BC%89/">特征选择和特征理解 （转）</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/23/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E7%89%B9%E5%BE%81%E7%90%86%E8%A7%A3%20%EF%BC%88%E8%BD%AC%EF%BC%89/" class="article-date"><time datetime="2016-12-22T16:00:00.000Z" itemprop="datePublished">2016-12-23</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">1 去掉取值变化小的特征 Removing features with low variance</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">2 单变量特征选择 Univariate feature selection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">2.1 Pearson相关系数 Pearson Correlation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">2.2 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">2.3 距离相关系数 (Distance correlation)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">2.4 基于学习模型的特征排序 (Model based ranking)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">3 线性模型和正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">3.1 正则化模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">3.2 L1正则化&#x2F;Lasso</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">3.3 L2正则化&#x2F;Ridge regression</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">4 随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">4.1 平均不纯度减少 mean decrease impurity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">4.2 平均精确率减少 Mean decrease accuracy</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">5 两种顶层特征选择算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">5.1 稳定性选择 Stability selection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-text">5.2 递归特征消除 Recursive feature elimination (RFE)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">6 一个完整的例子</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#"><span class="toc-text">Tips</span></a></li></ol>
</div>

        <p>作者：<a href="http://www.chaoslog.com/author/edwin-jarvis.html" target="_blank" rel="noopener">Edwin Jarvis</a></p>
<p>特征选择(排序)对于数据科学家、<a href="http://lib.csdn.net/base/2" target="_blank" rel="noopener">机器学习</a>从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、<a href="http://lib.csdn.net/base/31" target="_blank" rel="noopener">算法</a>都有着重要作用。</p>
<p>特征选择主要有两个功能：</p>
<ol>
<li>减少特征数量、降维，使模型泛化能力更强，减少过拟合</li>
<li>增强对特征和特征值之间的理解</li>
</ol>
<p>拿到数据集，一个特征选择方法，往往很难同时完成这两个目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）。</p>
<p>在许多机器学习相关的书里，很难找到关于特征选择的内容，因为特征选择要解决的问题往往被视为机器学习的一种副作用，一般不会单独拿出来讨论。</p>
<p>本文将结合<a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" target="_blank" rel="noopener">Scikit-learn提供的例子</a>介绍几种常用的特征选择方法，它们各自的优缺点和问题。</p>
<h1><span id="1-qu-diao-qu-zhi-bian-hua-xiao-de-te-zheng-removing-features-with-low-variance">1 去掉取值变化小的特征 Removing features with low variance</span><a href="#1-qu-diao-qu-zhi-bian-hua-xiao-de-te-zheng-removing-features-with-low-variance" class="header-anchor">#</a></h1><p>这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p>
<h1><span id="2-dan-bian-liang-te-zheng-xuan-ze-univariate-feature-selection">2 单变量特征选择 Univariate feature selection</span><a href="#2-dan-bian-liang-te-zheng-xuan-ze-univariate-feature-selection" class="header-anchor">#</a></h1><p>单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。</p>
<p>这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。</p>
<h2><span id="2-1-pearson-xiang-guan-xi-shu-pearson-correlation">2.1 Pearson相关系数 Pearson Correlation</span><a href="#2-1-pearson-xiang-guan-xi-shu-pearson-correlation" class="header-anchor">#</a></h2><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。</p>
<p>Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的<a href="http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html" target="_blank" rel="noopener">pearsonr</a>方法能够同时计算相关系数和p-value，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line">np.random.seed(0)</span><br><span class="line">size &#x3D; 300</span><br><span class="line">x &#x3D; np.random.normal(0, 1, size)</span><br><span class="line">print &quot;Lower noise&quot;, pearsonr(x, x + np.random.normal(0, 1, size))</span><br><span class="line">print &quot;Higher noise&quot;, pearsonr(x, x + np.random.normal(0, 10, size))</span><br></pre></td></tr></table></figure>

<p>Lower noise (0.71824836862138386, 7.3240173129992273e-49)<br><br>Higher noise (0.057964292079338148, 0.31700993885324746)</p>
<p>这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。</p>
<p>Scikit-learn提供的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" target="_blank" rel="noopener">f_regrssion</a>方法能够批量计算特征的p-value，非常方便，参考sklearn的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" target="_blank" rel="noopener">pipeline</a></p>
<p>Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.random.uniform(-1, 1, 100000)</span><br><span class="line">print pearsonr(x, x**2)[0]</span><br></pre></td></tr></table></figure>

<p>-0.00230804707612</p>
<p>更多类似的例子参考<a href="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/506px-Correlation_examples2.svg.png" target="_blank" rel="noopener">sample plots</a>。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如<a href="http://en.wikipedia.org/wiki/Anscombe%27s_quartet" target="_blank" rel="noopener">Anscombe&rsquo;s quartet</a>，最好把数据可视化出来，以免得出错误的结论。</p>
<h2><span id="2-2-hu-xin-xi-he-zui-da-xin-xi-xi-shu-mutual-information-and-maximal-information-coefficient-mic">2.2 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)</span><a href="#2-2-hu-xin-xi-he-zui-da-xin-xi-xi-shu-mutual-information-and-maximal-information-coefficient-mic" class="header-anchor">#</a></h2><p><a href="http://dataunion.org/wp-content/uploads/2015/04/4dbuAXe.png" target="_blank" rel="noopener"><img style="border: 0px; vertical-align: middle; max-width: 100%; height: auto;" src="http://dataunion.org/wp-content/uploads/2015/04/4dbuAXe.png" alt></a></p>
<p>以上就是经典的互信息公式了。想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。</p>
<p>最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。<a href="http://minepy.sourceforge.net/" target="_blank" rel="noopener">minepy</a>提供了MIC功能。</p>
<p>反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from minepy import MINE</span><br><span class="line">m &#x3D; MINE()</span><br><span class="line">x &#x3D; np.random.uniform(-1, 1, 10000)</span><br><span class="line">m.compute_score(x, x**2)</span><br><span class="line">print m.mic()</span><br></pre></td></tr></table></figure>

<p>1.0</p>
<p>MIC的统计能力遭到了<a href="http://statweb.stanford.edu/%7Etibs/reshef/comment.pdf" target="_blank" rel="noopener">一些质疑</a>，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。</p>
<h2><span id="2-3-ju-chi-xiang-guan-xi-shu-distance-correlation">2.3 距离相关系数 (Distance correlation)</span><a href="#2-3-ju-chi-xiang-guan-xi-shu-distance-correlation" class="header-anchor">#</a></h2><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。</p>
<p>R的<a href="http://cran.r-project.org/web/packages/energy/index.html" target="_blank" rel="noopener">energy</a>包里提供了距离相关系数的实现，另外这是<a href="https://gist.github.com/josef-pkt/2938402" target="_blank" rel="noopener">Python gist</a>的实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#R-code</span><br><span class="line">&gt; x &#x3D; runif (1000, -1, 1)</span><br><span class="line">&gt; dcor(x, x**2)</span><br><span class="line">[1] 0.4943864</span><br></pre></td></tr></table></figure>

<p>尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p>
<h2><span id="2-4-ji-yu-xue-xi-mo-xing-de-te-zheng-pai-xu-model-based-ranking">2.4 基于学习模型的特征排序 (Model based ranking)</span><a href="#2-4-ji-yu-xue-xi-mo-xing-de-te-zheng-pai-xu-model-based-ranking" class="header-anchor">#</a></h2><p>这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。</p>
<p>在<a href="https://archive.ics.uci.edu/ml/datasets/Housing" target="_blank" rel="noopener">波士顿房价数据集</a>上使用sklearn的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank" rel="noopener">随机森林回归</a>给出一个单变量选择的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import cross_val_score, ShuffleSplit</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line"></span><br><span class="line">#Load boston housing dataset as an example</span><br><span class="line">boston &#x3D; load_boston()</span><br><span class="line">X &#x3D; boston[&quot;data&quot;]</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line">names &#x3D; boston[&quot;feature_names&quot;]</span><br><span class="line"></span><br><span class="line">rf &#x3D; RandomForestRegressor(n_estimators&#x3D;20, max_depth&#x3D;4)</span><br><span class="line">scores &#x3D; []</span><br><span class="line">for i in range(X.shape[1]):</span><br><span class="line">     score &#x3D; cross_val_score(rf, X[:, i:i+1], Y, scoring&#x3D;&quot;r2&quot;,</span><br><span class="line">                              cv&#x3D;ShuffleSplit(len(X), 3, .3))</span><br><span class="line">     scores.append((round(np.mean(score), 3), names[i]))</span><br><span class="line">print sorted(scores, reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<p>[(0.636, &lsquo;LSTAT&rsquo;), (0.59, &lsquo;RM&rsquo;), (0.472, &lsquo;NOX&rsquo;), (0.369, &lsquo;INDUS&rsquo;), (0.311, &lsquo;PTRATIO&rsquo;), (0.24, &lsquo;TAX&rsquo;), (0.24, &lsquo;CRIM&rsquo;), (0.185, &lsquo;RAD&rsquo;), (0.16, &lsquo;ZN&rsquo;), (0.087, &lsquo;B&rsquo;), (0.062, &lsquo;DIS&rsquo;), (0.036, &lsquo;CHAS&rsquo;), (0.027, &lsquo;AGE&rsquo;)]</p>
<h1><span id="3-xian-xing-mo-xing-he-zheng-ze-hua">3 线性模型和正则化</span><a href="#3-xian-xing-mo-xing-he-zheng-ze-hua" class="header-anchor">#</a></h1><p>单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。</p>
<p>下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line">size &#x3D; 5000</span><br><span class="line"></span><br><span class="line">#A dataset with 3 features</span><br><span class="line">X &#x3D; np.random.normal(0, 1, (size, 3))</span><br><span class="line">#Y &#x3D; X0 + 2*X1 + noise</span><br><span class="line">Y &#x3D; X[:,0] + 2*X[:,1] + np.random.normal(0, 2, size)</span><br><span class="line">lr &#x3D; LinearRegression()</span><br><span class="line">lr.fit(X, Y)</span><br><span class="line"></span><br><span class="line">#A helper method for pretty-printing linear models</span><br><span class="line">def pretty_print_linear(coefs, names &#x3D; None, sort &#x3D; False):</span><br><span class="line">    if names &#x3D;&#x3D; None:</span><br><span class="line">        names &#x3D; [&quot;X%s&quot; % x for x in range(len(coefs))]</span><br><span class="line">    lst &#x3D; zip(coefs, names)</span><br><span class="line">    if sort:</span><br><span class="line">        lst &#x3D; sorted(lst,  key &#x3D; lambda x:-np.abs(x[0]))</span><br><span class="line">    return &quot; + &quot;.join(&quot;%s * %s&quot; % (round(coef, 3), name)</span><br><span class="line">                                   for coef, name in lst)</span><br><span class="line"></span><br><span class="line">print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_)</span><br></pre></td></tr></table></figure>

<p>Linear model: 0.984 * X0 + 1.995 * X1 + -0.041 * X2</p>
<p>在这个例子当中，尽管数据中存在一些噪音，但这种特征选择模型仍然能够很好的体现出数据的底层结构。当然这也是因为例子中的这个问题非常适合用线性模型来解：特征和响应变量之间全都是线性关系，并且特征之间均是独立的。</p>
<p>在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成W），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是Y=X1+X2，当我们观察的时候，发现Y&rsquo;=X1+X2+e，e是噪音。如果X1和X2之间存在线性关系，例如X1约等于X2，这个时候由于噪音e的存在，我们学到的模型可能就不是Y=X1+X2了，有可能是Y=2X1，或者Y=-X1+3X2。</p>
<p>下边这个例子当中，在同一个数据上加入了一些噪音，用随机森林算法进行特征选择。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">size &#x3D; 100</span><br><span class="line">np.random.seed(seed&#x3D;5)</span><br><span class="line"></span><br><span class="line">X_seed &#x3D; np.random.normal(0, 1, size)</span><br><span class="line">X1 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">X2 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">X3 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line"></span><br><span class="line">Y &#x3D; X1 + X2 + X3 + np.random.normal(0,1, size)</span><br><span class="line">X &#x3D; np.array([X1, X2, X3]).T</span><br><span class="line"></span><br><span class="line">lr &#x3D; LinearRegression()</span><br><span class="line">lr.fit(X,Y)</span><br><span class="line">print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_)</span><br></pre></td></tr></table></figure>

<p>Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2</p>
<p>系数之和接近3，基本上和上上个例子的结果一致，应该说学到的模型对于预测来说还是不错的。但是，如果从系数的字面意思上去解释特征的重要性的话，X3对于输出变量来说具有很强的正面影响，而X1具有负面影响，而实际上所有特征与输出变量之间的影响是均等的。</p>
<p>同样的方法和套路可以用到类似的线性模型上，比如逻辑回归。</p>
<h2><span id="3-1-zheng-ze-hua-mo-xing">3.1 正则化模型</span><a href="#3-1-zheng-ze-hua-mo-xing" class="header-anchor">#</a></h2><p>正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||&middot;||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。</p>
<h2><span id="3-2-l1-zheng-ze-hua-lasso">3.2 L1正则化/Lasso</span><a href="#3-2-l1-zheng-ze-hua-lasso" class="header-anchor">#</a></h2><p>L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。</p>
<p>Scikit-learn为线性回归提供了Lasso，为分类提供了L1逻辑回归。</p>
<p>下面的例子在波士顿房价数据上运行了Lasso，其中参数alpha是通过grid search进行优化的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line"></span><br><span class="line">boston &#x3D; load_boston()</span><br><span class="line">scaler &#x3D; StandardScaler()</span><br><span class="line">X &#x3D; scaler.fit_transform(boston[&quot;data&quot;])</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line">names &#x3D; boston[&quot;feature_names&quot;]</span><br><span class="line"></span><br><span class="line">lasso &#x3D; Lasso(alpha&#x3D;.3)</span><br><span class="line">lasso.fit(X, Y)</span><br><span class="line"></span><br><span class="line">print &quot;Lasso model: &quot;, pretty_print_linear(lasso.coef_, names, sort &#x3D; True)</span><br></pre></td></tr></table></figure>

<p>Lasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX</p>
<p>可以看到，很多特征的系数都是0。如果继续增加alpha的值，得到的模型就会越来越稀疏，即越来越多的特征系数会变成0。</p>
<p>然而，L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异。</p>
<h2><span id="3-3-l2-zheng-ze-hua-ridge-regression">3.3 L2正则化/Ridge regression</span><a href="#3-3-l2-zheng-ze-hua-ridge-regression" class="header-anchor">#</a></h2><p>L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。</p>
<p>可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。</p>
<p>回过头来看看3个互相关联的特征的例子，分别以10个不同的种子随机初始化运行10次，来观察L1和L2正则化的稳定性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">size &#x3D; 100</span><br><span class="line"></span><br><span class="line">#We run the method 10 times with different random seeds</span><br><span class="line">for i in range(10):</span><br><span class="line">    print &quot;Random seed %s&quot; % i</span><br><span class="line">    np.random.seed(seed&#x3D;i)</span><br><span class="line">    X_seed &#x3D; np.random.normal(0, 1, size)</span><br><span class="line">    X1 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">    X2 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">    X3 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">    Y &#x3D; X1 + X2 + X3 + np.random.normal(0, 1, size)</span><br><span class="line">    X &#x3D; np.array([X1, X2, X3]).T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    lr &#x3D; LinearRegression()</span><br><span class="line">    lr.fit(X,Y)</span><br><span class="line">    print &quot;Linear model:&quot;, pretty_print_linear(lr.coef_)</span><br><span class="line"></span><br><span class="line">    ridge &#x3D; Ridge(alpha&#x3D;10)</span><br><span class="line">    ridge.fit(X,Y)</span><br><span class="line">    print &quot;Ridge model:&quot;, pretty_print_linear(ridge.coef_)</span><br><span class="line">    print</span><br></pre></td></tr></table></figure>

<p>Random seed 0 Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2 Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2</p>
<p>Random seed 1 Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2 Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2</p>
<p>Random seed 2 Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2 Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2</p>
<p>Random seed 3 Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2 Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2</p>
<p>Random seed 4 Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2 Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2</p>
<p>Random seed 5 Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2 Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2</p>
<p>Random seed 6 Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2 Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2</p>
<p>Random seed 7 Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2 Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2</p>
<p>Random seed 8 Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2 Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2</p>
<p>Random seed 9 Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2 Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2</p>
<p>可以看出，不同的数据上线性回归得到的模型（系数）相差甚远，但对于L2正则化模型来说，结果中的系数非常的稳定，差别较小，都比较接近于1，能够反映出数据的内在结构。</p>
<h1><span id="4-sui-ji-sen-lin">4 随机森林</span><a href="#4-sui-ji-sen-lin" class="header-anchor">#</a></h1><p>随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。</p>
<h2><span id="4-1-ping-jun-bu-chun-du-jian-shao-mean-decrease-impurity">4.1 平均不纯度减少 mean decrease impurity</span><a href="#4-1-ping-jun-bu-chun-du-jian-shao-mean-decrease-impurity" class="header-anchor">#</a></h2><p>随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用<a href="http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" target="_blank" rel="noopener">基尼不纯度</a>或者<a href="http://en.wikipedia.org/wiki/Information_gain_in_decision_trees" target="_blank" rel="noopener">信息增益</a>，对于回归问题，通常采用的是<a href="http://en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener">方差</a>或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。</p>
<p>下边的例子是sklearn中基于随机森林的特征重要度度量方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">import numpy as np</span><br><span class="line">#Load boston housing dataset as an example</span><br><span class="line">boston &#x3D; load_boston()</span><br><span class="line">X &#x3D; boston[&quot;data&quot;]</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line">names &#x3D; boston[&quot;feature_names&quot;]</span><br><span class="line">rf &#x3D; RandomForestRegressor()</span><br><span class="line">rf.fit(X, Y)</span><br><span class="line">print &quot;Features sorted by their score:&quot;</span><br><span class="line">print sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), </span><br><span class="line">             reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<p>Features sorted by their score: [(0.5298, &lsquo;LSTAT&rsquo;), (0.4116, &lsquo;RM&rsquo;), (0.0252, &lsquo;DIS&rsquo;), (0.0172, &lsquo;CRIM&rsquo;), (0.0065, &lsquo;NOX&rsquo;), (0.0035, &lsquo;PTRATIO&rsquo;), (0.0021, &lsquo;TAX&rsquo;), (0.0017, &lsquo;AGE&rsquo;), (0.0012, &lsquo;B&rsquo;), (0.0008, &lsquo;INDUS&rsquo;), (0.0004, &lsquo;RAD&rsquo;), (0.0001, &lsquo;CHAS&rsquo;), (0.0, &lsquo;ZN&rsquo;)]</p>
<p>这里特征得分实际上采用的是<a href="http://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm#giniimp" target="_blank" rel="noopener">Gini Importance</a>。使用基于不纯度的方法的时候，要记住：1、这种方法存在<a href="http://link.springer.com/article/10.1186%2F1471-2105-8-25" target="_blank" rel="noopener">偏向</a>，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。</p>
<p><a href="http://en.wikipedia.org/wiki/Random_subspace_method" target="_blank" rel="noopener">特征随机选择</a>方法稍微缓解了这个问题，但总的来说并没有完全解决。下面的例子中，X0、X1、X2是三个互相关联的变量，在没有噪音的情况下，输出变量是三者之和。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">size &#x3D; 10000</span><br><span class="line">np.random.seed(seed&#x3D;10)</span><br><span class="line">X_seed &#x3D; np.random.normal(0, 1, size)</span><br><span class="line">X0 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">X1 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">X2 &#x3D; X_seed + np.random.normal(0, .1, size)</span><br><span class="line">X &#x3D; np.array([X0, X1, X2]).T</span><br><span class="line">Y &#x3D; X0 + X1 + X2</span><br><span class="line"></span><br><span class="line">rf &#x3D; RandomForestRegressor(n_estimators&#x3D;20, max_features&#x3D;2)</span><br><span class="line">rf.fit(X, Y);</span><br><span class="line">print &quot;Scores for X0, X1, X2:&quot;, map(lambda x:round (x,3),</span><br><span class="line">                                    rf.feature_importances_)</span><br></pre></td></tr></table></figure>

<p>Scores for X0, X1, X2: [0.278, 0.66, 0.062]</p>
<p>当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。</p>
<p>需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。</p>
<h2><span id="4-2-ping-jun-jing-que-lu-jian-shao-mean-decrease-accuracy">4.2 平均精确率减少 Mean decrease accuracy</span><a href="#4-2-ping-jun-jing-que-lu-jian-shao-mean-decrease-accuracy" class="header-anchor">#</a></h2><p>另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。</p>
<p>这个方法sklearn中没有直接提供，但是很容易实现，下面继续在波士顿房价数据集上进行实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import ShuffleSplit</span><br><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">X &#x3D; boston[&quot;data&quot;]</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line"></span><br><span class="line">rf &#x3D; RandomForestRegressor()</span><br><span class="line">scores &#x3D; defaultdict(list)</span><br><span class="line"></span><br><span class="line">#crossvalidate the scores on a number of different random splits of the data</span><br><span class="line">for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):</span><br><span class="line">    X_train, X_test &#x3D; X[train_idx], X[test_idx]</span><br><span class="line">    Y_train, Y_test &#x3D; Y[train_idx], Y[test_idx]</span><br><span class="line">    r &#x3D; rf.fit(X_train, Y_train)</span><br><span class="line">    acc &#x3D; r2_score(Y_test, rf.predict(X_test))</span><br><span class="line">    for i in range(X.shape[1]):</span><br><span class="line">        X_t &#x3D; X_test.copy()</span><br><span class="line">        np.random.shuffle(X_t[:, i])</span><br><span class="line">        shuff_acc &#x3D; r2_score(Y_test, rf.predict(X_t))</span><br><span class="line">        scores[names[i]].append((acc-shuff_acc)&#x2F;acc)</span><br><span class="line">print &quot;Features sorted by their score:&quot;</span><br><span class="line">print sorted([(round(np.mean(score), 4), feat) for</span><br><span class="line">              feat, score in scores.items()], reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<p>Features sorted by their score: [(0.7276, &lsquo;LSTAT&rsquo;), (0.5675, &lsquo;RM&rsquo;), (0.0867, &lsquo;DIS&rsquo;), (0.0407, &lsquo;NOX&rsquo;), (0.0351, &lsquo;CRIM&rsquo;), (0.0233, &lsquo;PTRATIO&rsquo;), (0.0168, &lsquo;TAX&rsquo;), (0.0122, &lsquo;AGE&rsquo;), (0.005, &lsquo;B&rsquo;), (0.0048, &lsquo;INDUS&rsquo;), (0.0043, &lsquo;RAD&rsquo;), (0.0004, &lsquo;ZN&rsquo;), (0.0001, &lsquo;CHAS&rsquo;)]</p>
<p>在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变。</p>
<h1><span id="5-liang-chong-ding-ceng-te-zheng-xuan-ze-suan-fa">5 两种顶层特征选择算法</span><a href="#5-liang-chong-ding-ceng-te-zheng-xuan-ze-suan-fa" class="header-anchor">#</a></h1><p>之所以叫做顶层，是因为他们都是建立在基于模型的特征选择方法基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。</p>
<h2><span id="5-1-wen-ding-xing-xuan-ze-stability-selection">5.1 稳定性选择 Stability selection</span><a href="#5-1-wen-ding-xing-xuan-ze-stability-selection" class="header-anchor">#</a></h2><p>稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</p>
<p>sklearn在<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html" target="_blank" rel="noopener">随机lasso</a>和<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html" target="_blank" rel="noopener">随机逻辑回归</a>中有对稳定性选择的实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import RandomizedLasso</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">boston &#x3D; load_boston()</span><br><span class="line"></span><br><span class="line">#using the Boston housing data. </span><br><span class="line">#Data gets scaled automatically by sklearn&#39;s implementation</span><br><span class="line">X &#x3D; boston[&quot;data&quot;]</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line">names &#x3D; boston[&quot;feature_names&quot;]</span><br><span class="line"></span><br><span class="line">rlasso &#x3D; RandomizedLasso(alpha&#x3D;0.025)</span><br><span class="line">rlasso.fit(X, Y)</span><br><span class="line"></span><br><span class="line">print &quot;Features sorted by their score:&quot;</span><br><span class="line">print sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), </span><br><span class="line">                 names), reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<p>Features sorted by their score: [(1.0, &lsquo;RM&rsquo;), (1.0, &lsquo;PTRATIO&rsquo;), (1.0, &lsquo;LSTAT&rsquo;), (0.62, &lsquo;CHAS&rsquo;), (0.595, &lsquo;B&rsquo;), (0.39, &lsquo;TAX&rsquo;), (0.385, &lsquo;CRIM&rsquo;), (0.25, &lsquo;DIS&rsquo;), (0.22, &lsquo;NOX&rsquo;), (0.125, &lsquo;INDUS&rsquo;), (0.045, &lsquo;ZN&rsquo;), (0.02, &lsquo;RAD&rsquo;), (0.015, &lsquo;AGE&rsquo;)]</p>
<p>在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。</p>
<h2><span id="5-2-di-gui-te-zheng-xiao-chu-recursive-feature-elimination-rfe">5.2 递归特征消除 Recursive feature elimination (RFE)</span><a href="#5-2-di-gui-te-zheng-xiao-chu-recursive-feature-elimination-rfe" class="header-anchor">#</a></h2><p>递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。</p>
<p>RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。</p>
<p>Sklearn提供了<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" target="_blank" rel="noopener">RFE</a>包，可以用于特征消除，还提供了<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" target="_blank" rel="noopener">RFECV</a>，可以通过交叉验证来对的特征进行排序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import RFE</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">boston &#x3D; load_boston()</span><br><span class="line">X &#x3D; boston[&quot;data&quot;]</span><br><span class="line">Y &#x3D; boston[&quot;target&quot;]</span><br><span class="line">names &#x3D; boston[&quot;feature_names&quot;]</span><br><span class="line"></span><br><span class="line">#use linear regression as the model</span><br><span class="line">lr &#x3D; LinearRegression()</span><br><span class="line">#rank all features, i.e continue the elimination until the last one</span><br><span class="line">rfe &#x3D; RFE(lr, n_features_to_select&#x3D;1)</span><br><span class="line">rfe.fit(X,Y)</span><br><span class="line"></span><br><span class="line">print &quot;Features sorted by their rank:&quot;</span><br><span class="line">print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names))</span><br></pre></td></tr></table></figure>

<p>Features sorted by their rank: [(1.0, &lsquo;NOX&rsquo;), (2.0, &lsquo;RM&rsquo;), (3.0, &lsquo;CHAS&rsquo;), (4.0, &lsquo;PTRATIO&rsquo;), (5.0, &lsquo;DIS&rsquo;), (6.0, &lsquo;LSTAT&rsquo;), (7.0, &lsquo;RAD&rsquo;), (8.0, &lsquo;CRIM&rsquo;), (9.0, &lsquo;INDUS&rsquo;), (10.0, &lsquo;ZN&rsquo;), (11.0, &lsquo;TAX&rsquo;), (12.0, &lsquo;B&rsquo;), (13.0, &lsquo;AGE&rsquo;)]</p>
<h1><span id="6-yi-ge-wan-zheng-de-li-zi">6 一个完整的例子</span><a href="#6-yi-ge-wan-zheng-de-li-zi" class="header-anchor">#</a></h1><p>下面将本文所有提到的方法进行实验对比，数据集采用Friedman #1 回归数据（<a href="ftp://ftp.uic.edu/pub/depts/econ/hhstokes/e538/Friedman_mars_1991.pdf">这篇论文</a>中的数据）。数据是用这个公式产生的：</p>
<p><a href="http://dataunion.org/wp-content/uploads/2015/04/ERGNG8c.png" target="_blank" rel="noopener"><img style="border: 0px; vertical-align: middle; max-width: 100%; height: auto;" src="http://dataunion.org/wp-content/uploads/2015/04/ERGNG8c.png" alt></a></p>
<p>X1到X5是由<a href="http://en.wikipedia.org/wiki/Univariate_distribution" target="_blank" rel="noopener">单变量分布</a>生成的，e是<a href="http://en.wikipedia.org/wiki/Standard_normal_deviate" target="_blank" rel="noopener">标准正态变量</a>N(0,1)。另外，原始的数据集中含有5个噪音变量 X5,&hellip;,X10，跟响应变量是独立的。我们增加了4个额外的变量X11,&hellip;X14，分别是X1,&hellip;,X4的关联变量，通过f(x)=x+N(0,0.01)生成，这将产生大于0.999的关联系数。这样生成的数据能够体现出不同的特征排序方法应对关联特征时的表现。</p>
<p>接下来将会在上述数据上运行所有的特征选择方法，并且将每种方法给出的得分进行归一化，让取值都落在0-1之间。对于RFE来说，由于它给出的是顺序而不是得分，我们将最好的5个的得分定为1，其他的特征的得分均匀的分布在0-1之间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">from sklearn.linear_model import (LinearRegression, Ridge, </span><br><span class="line">                                  Lasso, RandomizedLasso)</span><br><span class="line">from sklearn.feature_selection import RFE, f_regression</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">import numpy as np</span><br><span class="line">from minepy import MINE</span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line"></span><br><span class="line">size &#x3D; 750</span><br><span class="line">X &#x3D; np.random.uniform(0, 1, (size, 14))</span><br><span class="line"></span><br><span class="line">#&quot;Friedamn #1&amp;rdquo; regression problem</span><br><span class="line">Y &#x3D; (10 * np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2] - .5)**2 +</span><br><span class="line">     10*X[:,3] + 5*X[:,4] + np.random.normal(0,1))</span><br><span class="line">#Add 3 additional correlated variables (correlated with X1-X3)</span><br><span class="line">X[:,10:] &#x3D; X[:,:4] + np.random.normal(0, .025, (size,4))</span><br><span class="line"></span><br><span class="line">names &#x3D; [&quot;x%s&quot; % i for i in range(1,15)]</span><br><span class="line"></span><br><span class="line">ranks &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">def rank_to_dict(ranks, names, order&#x3D;1):</span><br><span class="line">    minmax &#x3D; MinMaxScaler()</span><br><span class="line">    ranks &#x3D; minmax.fit_transform(order*np.array([ranks]).T).T[0]</span><br><span class="line">    ranks &#x3D; map(lambda x: round(x, 2), ranks)</span><br><span class="line">    return dict(zip(names, ranks ))</span><br><span class="line"></span><br><span class="line">lr &#x3D; LinearRegression(normalize&#x3D;True)</span><br><span class="line">lr.fit(X, Y)</span><br><span class="line">ranks[&quot;Linear reg&quot;] &#x3D; rank_to_dict(np.abs(lr.coef_), names)</span><br><span class="line"></span><br><span class="line">ridge &#x3D; Ridge(alpha&#x3D;7)</span><br><span class="line">ridge.fit(X, Y)</span><br><span class="line">ranks[&quot;Ridge&quot;] &#x3D; rank_to_dict(np.abs(ridge.coef_), names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lasso &#x3D; Lasso(alpha&#x3D;.05)</span><br><span class="line">lasso.fit(X, Y)</span><br><span class="line">ranks[&quot;Lasso&quot;] &#x3D; rank_to_dict(np.abs(lasso.coef_), names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rlasso &#x3D; RandomizedLasso(alpha&#x3D;0.04)</span><br><span class="line">rlasso.fit(X, Y)</span><br><span class="line">ranks[&quot;Stability&quot;] &#x3D; rank_to_dict(np.abs(rlasso.scores_), names)</span><br><span class="line"></span><br><span class="line">#stop the search when 5 features are left (they will get equal scores)</span><br><span class="line">rfe &#x3D; RFE(lr, n_features_to_select&#x3D;5)</span><br><span class="line">rfe.fit(X,Y)</span><br><span class="line">ranks[&quot;RFE&quot;] &#x3D; rank_to_dict(map(float, rfe.ranking_), names, order&#x3D;-1)</span><br><span class="line"></span><br><span class="line">rf &#x3D; RandomForestRegressor()</span><br><span class="line">rf.fit(X,Y)</span><br><span class="line">ranks[&quot;RF&quot;] &#x3D; rank_to_dict(rf.feature_importances_, names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f, pval  &#x3D; f_regression(X, Y, center&#x3D;True)</span><br><span class="line">ranks[&quot;Corr.&quot;] &#x3D; rank_to_dict(f, names)</span><br><span class="line"></span><br><span class="line">mine &#x3D; MINE()</span><br><span class="line">mic_scores &#x3D; []</span><br><span class="line">for i in range(X.shape[1]):</span><br><span class="line">    mine.compute_score(X[:,i], Y)</span><br><span class="line">    m &#x3D; mine.mic()</span><br><span class="line">    mic_scores.append(m)</span><br><span class="line"></span><br><span class="line">ranks[&quot;MIC&quot;] &#x3D; rank_to_dict(mic_scores, names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">r &#x3D; &#123;&#125;</span><br><span class="line">for name in names:</span><br><span class="line">    r[name] &#x3D; round(np.mean([ranks[method][name] </span><br><span class="line">                             for method in ranks.keys()]), 2)</span><br><span class="line"></span><br><span class="line">methods &#x3D; sorted(ranks.keys())</span><br><span class="line">ranks[&quot;Mean&quot;] &#x3D; r</span><br><span class="line">methods.append(&quot;Mean&quot;)</span><br><span class="line"></span><br><span class="line">print &quot;\t%s&quot; % &quot;\t&quot;.join(methods)</span><br><span class="line">for name in names:</span><br><span class="line">    print &quot;%s\t%s&quot; % (name, &quot;\t&quot;.join(map(str, </span><br><span class="line">                         [ranks[method][name] for method in methods])))</span><br></pre></td></tr></table></figure>

<p><a href="http://dataunion.org/wp-content/uploads/2015/04/5ybQKSP.png" target="_blank" rel="noopener"><img style="border: 0px; vertical-align: middle; max-width: 100%; height: auto;" src="http://dataunion.org/wp-content/uploads/2015/04/5ybQKSP.png" alt></a></p>
<p>从以上结果中可以找到一些有趣的发现：</p>
<p>特征之间存在线性关联关系，每个特征都是独立评价的，因此X1,&hellip;X4的得分和X11,&hellip;X14的得分非常接近，而噪音特征X5,&hellip;,X10正如预期的那样和响应变量之间几乎没有关系。由于变量X3是二次的，因此X3和响应变量之间看不出有关系（除了MIC之外，其他方法都找不到关系）。这种方法能够衡量出特征和响应变量之间的线性关系，但若想选出优质特征来提升模型的泛化能力，这种方法就不是特别给力了，因为所有的优质特征都不可避免的会被挑出来两次。</p>
<p>Lasso能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。（例如在结果表中，X11,X12,X13的得分都是0，好像他们跟输出变量之间没有很强的联系，但实际上不是这样的）</p>
<p>MIC对特征一视同仁，这一点上和关联系数有点像，另外，它能够找出X3和响应变量之间的非线性关系。</p>
<p>随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。从表中可以看到，得分第三的特征比第一的小4倍。而其他的特征选择算法就没有下降的这么剧烈。</p>
<p>Ridge将回归系数均匀的分摊到各个关联变量上，从表中可以看出，X11,&hellip;,X14和X1,&hellip;,X4的得分非常接近。</p>
<p>稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择，在结果表中就能很好的看出。像Lasso一样，它能找到那些性能比较好的特征（X1，X2，X4，X5），同时，与这些特征关联度很强的变量也得到了较高的得分。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor">#</a></h1><ol>
<li>对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。</li>
<li>正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。</li>
<li>随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。</li>
<li>特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。</li>
</ol>
<h1><span id="tips">Tips</span><a href="#tips" class="header-anchor">#</a></h1><p>什么是<a href="http://en.wikipedia.org/wiki/Chi-square_test" target="_blank" rel="noopener">卡方检验</a>？用方差来衡量某个观测频率和理论频率之间差异性的方法</p>
<p>什么是<a href="http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test" target="_blank" rel="noopener">皮尔森卡方检验</a>？这是一种最常用的卡方检验方法，它有两个用途：1是计算某个变量对某种分布的拟合程度，2是根据两个观测变量的<a href="http://en.wikipedia.org/wiki/Contingency_table" target="_blank" rel="noopener">Contingency table</a>来计算这两个变量是否是独立的。主要有三个步骤：第一步用方差和的方式来计算观测频率和理论频率之间卡方值；第二步算出卡方检验的自由度（行数-1乘以列数-1）；第三步比较卡方值和对应自由度的卡方分布，判断显著性。</p>
<p>什么是<a href="http://en.wikipedia.org/wiki/P-value" target="_blank" rel="noopener">p-value</a>？简单地说，p-value就是为了验证假设和实际之间一致性的统计学意义的值，即假设检验。有些地方叫右尾概率，根据卡方值和自由度可以算出一个固定的p-value，</p>
<p>什么是<a href="http://www.answers.com/Q/What_is_a_response_variable" target="_blank" rel="noopener">响应变量(response value)</a>？简单地说，模型的输入叫做explanatroy variables，模型的输出叫做response variables，其实就是要验证该特征对结果造成了什么样的影响</p>
<p>什么是<a href="http://en.wikipedia.org/wiki/Statistical_power" target="_blank" rel="noopener">统计能力(statistical power)</a>?</p>
<p>什么是<a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29" target="_blank" rel="noopener">度量(metric)</a>?</p>
<p>什么是<a href="http://zh.wikipedia.org/wiki/%E9%9B%B6%E5%81%87%E8%AE%BE" target="_blank" rel="noopener">零假设(null hypothesis)</a>?在相关性检验中，一般会取&ldquo;两者之间无关联&rdquo;作为零假设，而在独立性检验中，一般会取&ldquo;两者之间是独立&rdquo;作为零假设。与零假设相对的是备择假设（对立假设），即希望证明是正确的另一种可能。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/23/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E7%89%B9%E5%BE%81%E7%90%86%E8%A7%A3%20%EF%BC%88%E8%BD%AC%EF%BC%89/" data-id="ckca3uy2k00drw20lgjlu9pkb" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习——预测数值型数据：回归" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/">机器学习——预测数值型数据：回归</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/" class="article-date"><time datetime="2016-12-20T16:00:00.000Z" itemprop="datePublished">2016-12-21</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>线性回归</strong></p>
<p><strong>优点</strong>：结果易于理解，计算上不复杂</p>
<p><strong>缺点</strong>：对非线性的数据拟合不好</p>
<p><strong>适用数据类型</strong>：数值型和标称型数据</p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%9B%9E%E5%BD%92/" data-id="ckca3uy2i00dlw20lg182alai" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习——非均衡分类问题" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">机器学习——非均衡分类问题</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" class="article-date"><time datetime="2016-12-18T16:00:00.000Z" itemprop="datePublished">2016-12-19</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>在机器学习的分类问题中，我们都假设所有类别的<strong>分类代价</strong>是一样的。但是事实上，不同分类的代价是不一样的，比如我们通过一个用于检测患病的系统来检测马匹是否能继续存活，如果我们把能存活的马匹检测成患病，那么这匹马可能就会被执行安乐死；如果我们把不能存活的马匹检测成健康，那么就会继续喂养这匹马。一个代价是错杀一只昂贵的动物，一个代价是继续喂养，很明显这<strong>两个代价是不一样的</strong>。</p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E5%9D%87%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" data-id="ckca3uy2i00dkw20l2ngk24f0" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-git命令" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/15/git%E5%91%BD%E4%BB%A4/">git命令</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/15/git%E5%91%BD%E4%BB%A4/" class="article-date"><time datetime="2016-12-14T16:00:00.000Z" itemprop="datePublished">2016-12-15</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>cd到需要git的目录</p>
<p>初始化git仓库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add origin git@github.com:Lintong-common&#x2F;XXX.git</span><br></pre></td></tr></table></figure>

<p>新建分支</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b testing</span><br></pre></td></tr></table></figure>

<p>添加并转到testing分支，不要直接在master分支上操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d testing</span><br></pre></td></tr></table></figure>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/15/git%E5%91%BD%E4%BB%A4/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/15/git%E5%91%BD%E4%BB%A4/" data-id="ckca3uy1r00b9w20l4nba50s0" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Git/" rel="tag">Git</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-ubuntu下非root用户下获得使用wireshark的权限" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/ubuntu%E4%B8%8B%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E4%B8%8B%E8%8E%B7%E5%BE%97%E4%BD%BF%E7%94%A8wireshark%E7%9A%84%E6%9D%83%E9%99%90/">ubuntu下非root用户下获得使用wireshark的权限</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/14/ubuntu%E4%B8%8B%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E4%B8%8B%E8%8E%B7%E5%BE%97%E4%BD%BF%E7%94%A8wireshark%E7%9A%84%E6%9D%83%E9%99%90/" class="article-date"><time datetime="2016-12-13T16:00:00.000Z" itemprop="datePublished">2016-12-14</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>在非root用户下不能使用wireshark用来抓包，所以需要进行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd  wireshark</span><br><span class="line">sudo chgrp wireshark &#x2F;usr&#x2F;bin&#x2F;dumpcap</span><br><span class="line">sudo chmod 4755 &#x2F;usr&#x2F;bin&#x2F;dumpcap</span><br><span class="line">sudo gpasswd -a common wireshark</span><br></pre></td></tr></table></figure>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/14/ubuntu%E4%B8%8B%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E4%B8%8B%E8%8E%B7%E5%BE%97%E4%BD%BF%E7%94%A8wireshark%E7%9A%84%E6%9D%83%E9%99%90/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/14/ubuntu%E4%B8%8B%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E4%B8%8B%E8%8E%B7%E5%BE%97%E4%BD%BF%E7%94%A8wireshark%E7%9A%84%E6%9D%83%E9%99%90/" data-id="ckca3uy1z00c1w20lhahkg6vp" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TCP-IP/" rel="tag">TCP/IP</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-网络抓包wireshark（转）" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85wireshark%EF%BC%88%E8%BD%AC%EF%BC%89/">网络抓包wireshark（转）</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/14/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85wireshark%EF%BC%88%E8%BD%AC%EF%BC%89/" class="article-date"><time datetime="2016-12-13T16:00:00.000Z" itemprop="datePublished">2016-12-14</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>wireshark的官方下载网站： <a href="http://www.wireshark.org/" target="_blank" rel="noopener">http://www.wireshark.org/</a></p>
<p>wireshark是非常流行的网络封包分析软件，功能十分强大。可以截取各种网络封包，显示网络封包的详细信息。</p>
<p>wireshark是开源软件，可以放心使用。 可以运行在Windows和Mac OS上。</p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/14/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85wireshark%EF%BC%88%E8%BD%AC%EF%BC%89/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/14/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85wireshark%EF%BC%88%E8%BD%AC%EF%BC%89/" data-id="ckca3uy2m00dzw20l3dekar8z" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TCP-IP/" rel="tag">TCP/IP</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-面试题总结" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/13/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/">面试题总结</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2016/12/13/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/" class="article-date"><time datetime="2016-12-12T16:00:00.000Z" itemprop="datePublished">2016-12-13</time></a>
</div>

    <div class="article-author">tonglin0325</div>
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>建一个做过的算法题<strong>目录</strong>，方便复习</p>
        
          <p class="article-more-link">
            <a class="btn btn-primary" href="/2016/12/13/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/#more">全文 &gt;&gt;</a>
          </p>
        
      
    </div>

    

    <footer class="article-footer">
      <a data-url="http://tonglin0325.github.io/2016/12/13/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/" data-id="ckimw91pb0006wo0l434cew0p" class="article-share-link">
        <i class="fa fa-share"></i> 分享
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li></ul>


    </footer>
  </div>
  
</article>



  


  <div id="page-nav">
    <nav><ul class="pagination"><li><a class="page-prev" rel="prev" href="/page/21/"><i class="fa fa-chevron-left"></i> Prev</a></li><li><a class="page-number" href="/">1</a></li><li class="disabled"><span class="page-space">&hellip;</span></li><li><a class="page-number" href="/page/20/">20</a></li><li><a class="page-number" href="/page/21/">21</a></li><li class="active"><span class="page-number">22</span></li><li><a class="page-number" href="/page/23/">23</a></li><li><a class="page-number" href="/page/24/">24</a></li><li class="disabled"><span class="page-space">&hellip;</span></li><li><a class="page-number" href="/page/57/">57</a></li><li><a class="page-next" rel="next" href="/page/23/">Next <i class="fa fa-chevron-right"></i></a></li></ul></nav>
  </div>



        </div>
        <div class="col-sm-3 col-sm-offset-0 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>tonglin0325</h4>
  <div style="text-align:center;"><img src="/images/WechatIMG1.jpeg" width="150" height="150" style="vertical-align:middle;"/></div>
</div>


  


  
  <div class="sidebar-module">
    <h4>标签</h4>
    <ul class="sidebar-module-list" itemprop="keywords"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Airbnb/" rel="tag">Airbnb</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Ajax/" rel="tag">Ajax</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Amazon/" rel="tag">Amazon</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Android/" rel="tag">Android</a><span class="sidebar-module-list-count">17</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Bootstrap/" rel="tag">Bootstrap</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ELK/" rel="tag">ELK</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Git/" rel="tag">Git</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HTML-CSS-JS/" rel="tag">HTML/CSS/JS</a><span class="sidebar-module-list-count">32</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hbase/" rel="tag">Hbase</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JDBC/" rel="tag">JDBC</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JSON/" rel="tag">JSON</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JVM/" rel="tag">JVM</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JavaSE/" rel="tag">JavaSE</a><span class="sidebar-module-list-count">76</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/JavaWeb/" rel="tag">JavaWeb</a><span class="sidebar-module-list-count">38</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Java%E5%85%B3%E9%94%AE%E5%AD%97/" rel="tag">Java关键字</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Java%E7%95%8C%E9%9D%A2/" rel="tag">Java界面</a><span class="sidebar-module-list-count">16</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/LaTex/" rel="tag">LaTex</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Linkedin/" rel="tag">Linkedin</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="sidebar-module-list-count">57</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ML/" rel="tag">ML</a><span class="sidebar-module-list-count">28</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Maven/" rel="tag">Maven</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MySQL/" rel="tag">MySQL</a><span class="sidebar-module-list-count">23</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Nexus/" rel="tag">Nexus</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NoSQL/" rel="tag">NoSQL</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Paper/" rel="tag">Paper</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Python/" rel="tag">Python</a><span class="sidebar-module-list-count">20</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Reading/" rel="tag">Reading</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Scala/" rel="tag">Scala</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Solr/" rel="tag">Solr</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Spark/" rel="tag">Spark</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SpringMVC/" rel="tag">SpringMVC</a><span class="sidebar-module-list-count">13</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/TCP-IP/" rel="tag">TCP/IP</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Thrift/" rel="tag">Thrift</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/antlr/" rel="tag">antlr</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/cdh/" rel="tag">cdh</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/docker/" rel="tag">docker</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/filebeat/" rel="tag">filebeat</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/flume/" rel="tag">flume</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/go/" rel="tag">go</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/google/" rel="tag">google</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/jquery/" rel="tag">jquery</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/kerberos/" rel="tag">kerberos</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/ldap/" rel="tag">ldap</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/mac/" rel="tag">mac</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/nginx/" rel="tag">nginx</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/nlp/" rel="tag">nlp</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/open-falcon/" rel="tag">open-falcon</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/redis/" rel="tag">redis</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/twitter/" rel="tag">twitter</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/yarn/" rel="tag">yarn</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%9B%BE%E5%AD%98%E5%82%A8%E5%8F%8A%E8%AE%A1%E7%AE%97/" rel="tag">图存储及计算</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/" rel="tag">开发工具</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a><span class="sidebar-module-list-count">22</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E6%9D%82%E8%B0%88/" rel="tag">杂谈</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" rel="tag">正则表达式</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="sidebar-module-list-count">27</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a><span class="sidebar-module-list-count">7</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>标签云</h4>
    <p class="tagcloud">
      <a href="/tags/Airbnb/" style="font-size: 12.5px;">Airbnb</a> <a href="/tags/Ajax/" style="font-size: 10px;">Ajax</a> <a href="/tags/Amazon/" style="font-size: 10px;">Amazon</a> <a href="/tags/Android/" style="font-size: 15.83px;">Android</a> <a href="/tags/Bootstrap/" style="font-size: 10px;">Bootstrap</a> <a href="/tags/ELK/" style="font-size: 13.75px;">ELK</a> <a href="/tags/Flink/" style="font-size: 14.17px;">Flink</a> <a href="/tags/Git/" style="font-size: 10.83px;">Git</a> <a href="/tags/HTML-CSS-JS/" style="font-size: 18.75px;">HTML/CSS/JS</a> <a href="/tags/Hadoop/" style="font-size: 12.5px;">Hadoop</a> <a href="/tags/Hbase/" style="font-size: 11.67px;">Hbase</a> <a href="/tags/Hive/" style="font-size: 12.5px;">Hive</a> <a href="/tags/JDBC/" style="font-size: 13.33px;">JDBC</a> <a href="/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/tags/JVM/" style="font-size: 10.42px;">JVM</a> <a href="/tags/JavaSE/" style="font-size: 20px;">JavaSE</a> <a href="/tags/JavaWeb/" style="font-size: 19.17px;">JavaWeb</a> <a href="/tags/Java%E5%85%B3%E9%94%AE%E5%AD%97/" style="font-size: 12.92px;">Java关键字</a> <a href="/tags/Java%E7%95%8C%E9%9D%A2/" style="font-size: 15.42px;">Java界面</a> <a href="/tags/LaTex/" style="font-size: 10px;">LaTex</a> <a href="/tags/Linkedin/" style="font-size: 10px;">Linkedin</a> <a href="/tags/Linux/" style="font-size: 19.58px;">Linux</a> <a href="/tags/ML/" style="font-size: 18.33px;">ML</a> <a href="/tags/Maven/" style="font-size: 10.83px;">Maven</a> <a href="/tags/MySQL/" style="font-size: 17.5px;">MySQL</a> <a href="/tags/Nexus/" style="font-size: 11.25px;">Nexus</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/Paper/" style="font-size: 10px;">Paper</a> <a href="/tags/Python/" style="font-size: 16.25px;">Python</a> <a href="/tags/Reading/" style="font-size: 10.83px;">Reading</a> <a href="/tags/Scala/" style="font-size: 13.33px;">Scala</a> <a href="/tags/Solr/" style="font-size: 10.83px;">Solr</a> <a href="/tags/Spark/" style="font-size: 16.67px;">Spark</a> <a href="/tags/SpringBoot/" style="font-size: 10.42px;">SpringBoot</a> <a href="/tags/SpringMVC/" style="font-size: 14.58px;">SpringMVC</a> <a href="/tags/TCP-IP/" style="font-size: 10.83px;">TCP/IP</a> <a href="/tags/Thrift/" style="font-size: 10.83px;">Thrift</a> <a href="/tags/antlr/" style="font-size: 10.42px;">antlr</a> <a href="/tags/cdh/" style="font-size: 10.83px;">cdh</a> <a href="/tags/docker/" style="font-size: 10.42px;">docker</a> <a href="/tags/filebeat/" style="font-size: 10.83px;">filebeat</a> <a href="/tags/flume/" style="font-size: 10.42px;">flume</a> <a href="/tags/go/" style="font-size: 10px;">go</a> <a href="/tags/google/" style="font-size: 10.42px;">google</a> <a href="/tags/jquery/" style="font-size: 10px;">jquery</a> <a href="/tags/kafka/" style="font-size: 12.92px;">kafka</a> <a href="/tags/kerberos/" style="font-size: 10px;">kerberos</a> <a href="/tags/ldap/" style="font-size: 10px;">ldap</a> <a href="/tags/mac/" style="font-size: 10.83px;">mac</a> <a href="/tags/nginx/" style="font-size: 12.08px;">nginx</a> <a href="/tags/nlp/" style="font-size: 10.42px;">nlp</a> <a href="/tags/open-falcon/" style="font-size: 10.83px;">open-falcon</a> <a href="/tags/redis/" style="font-size: 10.83px;">redis</a> <a href="/tags/twitter/" style="font-size: 10.42px;">twitter</a> <a href="/tags/yarn/" style="font-size: 10.42px;">yarn</a> <a href="/tags/zookeeper/" style="font-size: 13.75px;">zookeeper</a> <a href="/tags/%E5%88%B7%E9%A2%98/" style="font-size: 10px;">刷题</a> <a href="/tags/%E5%9B%BE%E5%AD%98%E5%82%A8%E5%8F%8A%E8%AE%A1%E7%AE%97/" style="font-size: 10.42px;">图存储及计算</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 13.75px;">多线程</a> <a href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/" style="font-size: 15px;">开发工具</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 17.08px;">数据结构</a> <a href="/tags/%E6%9D%82%E8%B0%88/" style="font-size: 12.08px;">杂谈</a> <a href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 10px;">正则表达式</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 17.92px;">算法</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 12.5px;">设计模式</a>
    </p>
  </div>



  
  <div class="sidebar-module">
    <h4>文章归档</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/01/">一月 2021</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/12/">十二月 2020</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/11/">十一月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/10/">十月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/09/">九月 2020</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/08/">八月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/07/">七月 2020</a><span class="sidebar-module-list-count">12</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/06/">六月 2020</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/05/">五月 2020</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/04/">四月 2020</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/03/">三月 2020</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/02/">二月 2020</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/01/">一月 2020</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/10/">十月 2019</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/09/">九月 2019</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/08/">八月 2019</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/07/">七月 2019</a><span class="sidebar-module-list-count">8</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/04/">四月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/03/">三月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/01/">一月 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/12/">十二月 2018</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/11/">十一月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/09/">九月 2018</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/06/">六月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/05/">五月 2018</a><span class="sidebar-module-list-count">10</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/04/">四月 2018</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/02/">二月 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/01/">一月 2018</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/11/">十一月 2017</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/09/">九月 2017</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/07/">七月 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">六月 2017</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">五月 2017</a><span class="sidebar-module-list-count">13</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/04/">四月 2017</a><span class="sidebar-module-list-count">24</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/03/">三月 2017</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/01/">一月 2017</a><span class="sidebar-module-list-count">24</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/12/">十二月 2016</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/11/">十一月 2016</a><span class="sidebar-module-list-count">20</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/10/">十月 2016</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/09/">九月 2016</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/08/">八月 2016</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/07/">七月 2016</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/06/">六月 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/05/">五月 2016</a><span class="sidebar-module-list-count">32</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/04/">四月 2016</a><span class="sidebar-module-list-count">27</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/03/">三月 2016</a><span class="sidebar-module-list-count">140</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/02/">二月 2016</a><span class="sidebar-module-list-count">22</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/09/">九月 2015</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/08/">八月 2015</a><span class="sidebar-module-list-count">13</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/07/">七月 2015</a><span class="sidebar-module-list-count">31</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/06/">六月 2015</a><span class="sidebar-module-list-count">25</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/04/">四月 2015</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/04/">四月 2013</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>最近文章</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2021/01/22/ElasticSearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/">ElasticSearch学习笔记——插件开发</a>
        </li>
      
        <li>
          <a href="/2021/01/21/Hbase%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94rowkey/">Hbase学习笔记——rowkey</a>
        </li>
      
        <li>
          <a href="/2021/01/20/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94fetch/">Hive学习笔记——fetch</a>
        </li>
      
        <li>
          <a href="/2021/01/11/kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%85%8D%E7%BD%AE/">kafka学习笔记——配置</a>
        </li>
      
        <li>
          <a href="/2021/01/07/ElasticSearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ik%E5%88%86%E8%AF%8D%E6%B7%BB%E5%8A%A0%E8%AF%8D%E5%BA%93/">ElasticSearch学习笔记——ik分词添加词库</a>
        </li>
      
    </ul>
  </div>




        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2021 tonglin0325<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>



<script src="/js/jquery.qrcode.min.js"></script>



<script src="/js/jquery-1.6.2.min.js"></script>


</body>
</html>
